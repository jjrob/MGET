name: Test data products

on:
  schedule:
    - cron: "41 16 * * *"  # Runs every day at 4:41 PM UTC

  workflow_run:
    workflows: ["Build and test wheels"]
    types: [completed]
    conclusion: success

  workflow_dispatch:  # Allows manual trigger

jobs:
  test_data_products_linux:
    name: Test data products on Linux
    runs-on: ubuntu-24.04

    environment:
      name: "Test data products"

    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Display system information
        run: |
          echo "uname -a:"
          uname -a
          echo ""
          echo "hostnamectl:"
          hostnamectl
          echo ""
          echo "python3 --version:"
          python3 --version

      - name: Display GDAL versions available for installation
        run: |
          apt-cache policy gdal-bin gdal-data gdal-plugins libgdal-dev 

      # Now we need to install GDAL. Unfortunately, we cannot use
      # awalsh128/cache-apt-pkgs-action for this, at least on ubuntu-24.04,
      # because it appears that GDAL has a post-install script that sets
      # LD_LIBRARY_PATH, and that script does not appear to run even if
      # execute_install_scripts is true for cache-apt-pkgs-action. At least, I
      # THINK that is the problem. In any case, if we use
      # cache-apt-pkgs-action, we get the following error when we try to run
      # gdalinfo later below:
      #
      #     gdalinfo: error while loading shared libraries: libblas.so.3: cannot open shared object file: No such file or directory
      #
      # So call apt-get ourselves. The line with curl installs apt-fast.

      - name: Install apt-fast
        run: |
          sudo /bin/bash -c "$(curl -sL https://git.io/vokNn)"
          sudo apt-get update -y

      - name: Install GDAL on ubuntu-24.04
        run: |
          sudo apt-fast install -y gdal-bin gdal-data gdal-plugins libgdal-dev libgdal34t64

      - name: Install Python packages needed to install GDAL's Python bindings
        run: |
          python -m pip install --upgrade pip
          python -m pip install "numpy<2" setuptools wheel

      - name: Install GDAL's Python package
        run: |
          GDAL_VERSION_NEEDED=$(gdalinfo --version | awk '{print $2}' | sed 's/,//')
          python -m pip install gdal==$GDAL_VERSION_NEEDED
          echo ""
          echo "Testing GDAL's python bindings"
          python -c "from osgeo import _gdal_array"

      - name: Get latest run of build-wheels.yml workflow on main branch
        id: get_run_id
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          workflow_file_name="build-wheels.yml"
          # Get the latest workflow run ID from the main branch; supposedly these are returned in reverse chronological order 
          workflow_runs=$(gh api repos/${{ github.repository }}/actions/workflows/$workflow_file_name/runs --paginate --jq '.workflow_runs[] | select(.head_branch == "main") | .id' | head -n 1)
          echo "RUN_ID: $workflow_runs"
          echo "RUN_ID=$workflow_runs" >> "$GITHUB_OUTPUT"

      - name: Download built wheels
        uses: actions/download-artifact@v4
        with:
          name: cibw-wheels-ubuntu-latest
          path: ./wheels
          run-id: ${{ steps.get_run_id.outputs.RUN_ID }}
          github-token: ${{ github.token }}    # Needed to download from another workflow run than the current run

      - name: Install wheel
        run: |
          GEOECO_VERSION=$(ls ./wheels | grep -m 1 mget3 | sed 's/.*mget3-\(.*\)-cp.*-cp.*.whl/\1/')
          python -m pip install mget3==$GEOECO_VERSION --find-links=./wheels

      - name: Install Python packages needed to run tests
        run: |
          python -m pip install pytest python-dotenv

      - name: Checkout source
        uses: actions/checkout@v4

      - name: Run tests
        run: |
          python -m pytest --junitxml=linux_results.xml ./test/GeoEco/DataProducts

      - name: Upload test linux_results.xml as artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux_results.xml
          path: linux_results.xml

      - name: Compute pass percentage
        run: |
          total_tests=$(grep -o 'tests="[0-9]*"' linux_results.xml | grep -o "[0-9]*")
          skipped_tests=$(grep -o 'skipped="[0-9]*"' linux_results.xml | grep -o "[0-9]*")
          failed_tests=$(grep -o 'failures="[0-9]*"' linux_results.xml | grep -o "[0-9]*")
          error_tests=$(grep -o 'errors="[0-9]*"' linux_results.xml | grep -o "[0-9]*")
          pass_percentage=$(awk "BEGIN {printf \"%.f\", (($total_tests-$skipped_tests-$failed_tests-$error_tests)/($total_tests-$skipped_tests))*100}")
          echo "PASS_PERCENTAGE: $pass_percentage%"
          echo "$pass_percentage" > pass_percentage.txt

      - name: Upload test pass percentage as artifact
        uses: actions/upload-artifact@v3
        with:
          name: linux_pass_percentage.txt
          path: linux_pass_percentage.txt
